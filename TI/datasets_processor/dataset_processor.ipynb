{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento\n",
    "1. En el dataset de Aeropuertos (A), se debe generar una columna llamada 'elevation_name' que contenga datos cualitativos indicando la elevación de cada aeropuerto. Esta columna se completará con las palabras: \"bajo\", \"medio\" o \"alto\", según la elevación de cada aeropuerto. Para definir estos grupos, se utilizarán los siguientes valores:\n",
    "######\n",
    "1. bajo: aeropuertos con elevación menor o igual a 131 ft.\n",
    "2. medio: aeropuertos con elevación mayor que 131 ft y menor o igual a 903 ft.\n",
    "3. altos: aeropuertos con elevación mayor a 903 ft.\n",
    "\n",
    "Además, se realizará otra modificación en este dataset, consistente en agregar una columna llamada 'prov_name', donde se incluirá el nombre de la provincia correspondiente a cada aeropuerto. Esta información se obtendrá consultando los nombres de las ciudades en el dataset (E).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "import dataset_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database design\n",
    "elevation_ft_field = 6\n",
    "\n",
    "read_airports = pathlib.Path('../datasets/original_datasets/ar-airports.csv')\n",
    "write_airports = pathlib.Path('../datasets/custom_datasets/ar-airports.csv')\n",
    "\n",
    "with(read_airports.open(mode='r', newline='', encoding='utf-8') as read_airports,\n",
    "     write_airports.open(mode='w', newline='', encoding='utf-8') as write_airports):\n",
    "    original_airports = csv.reader(read_airports)\n",
    "    custom_airports = csv.writer(write_airports)\n",
    "    #Create the header of the file, including the \"elevation_name\" and \"prov_name\" fields\n",
    "    header = next(original_airports)\n",
    "    header.append(\"elevation_name\")\n",
    "    header.append(\"prov_name\")\n",
    "    #Write header in custom dataset\n",
    "    custom_airports.writerow(header)\n",
    "    #Append elevation_name and prov_name for each airport\n",
    "    for line in original_airports:\n",
    "        #Include elevation to the list: elevation_ft = empty then elevation_name = empty\n",
    "        line.append(dataset_module.include_elevation(line))\n",
    "        #Include province to the list\n",
    "        line.append(dataset_module.include_prov(line))\n",
    "        #Write airport in custom dataset\n",
    "        custom_airports.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. En el dataset de Conectividad (B), se realizará un reemplazo en las celdas que contengan el carácter '-' con la palabra 'NO'.\n",
    "\n",
    "Para el mismo dataset se debe generar una nueva columna denominada 'posee_conectividad', la misma puede tomar uno de dos valores posibles: SÍ o NO. \n",
    "\n",
    "El valor será NO si todos los campos ADSL, CABLEMODEM, DIALUP, FIBRAOPTICA, SATELITAL, WIRELESS, TELEFONIAFIJA, 3G y 4G poseen el valor --. Caso contrario el valor será SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dataset = pathlib.Path('../datasets/original_datasets/Conectividad_Internet.csv')\n",
    "write_dataset = pathlib.Path('../datasets/custom_datasets/Conectividad_Internet.csv')\n",
    "\n",
    "with(read_dataset.open(mode='r', encoding='utf-8') as read_file,\n",
    "     write_dataset.open(mode='w', encoding='utf-8', newline='') as write_file):\n",
    "    \n",
    "    writer = csv.writer(write_file, delimiter=',')\n",
    "    writer.writerow(['Provincia', 'Partido', 'Localidad', 'Poblacion', 'ADSL', 'CABLEMODEM', 'DIALUP', 'FIBRAOPTICA', 'SATELITAL', 'WIRELESS', \n",
    "                     'TELEFONIAFIJA', '3G', '4G', 'link', 'Latitud', 'Longitud', 'posee_conectividad'])\n",
    "\n",
    "    reader = csv.DictReader(read_file, delimiter=',')\n",
    "    \n",
    "    lines = []\n",
    "    for line in reader:\n",
    "        line = dataset_module.has_connectivity(line)\n",
    "        line = dataset_module.hyphen_replace(line)\n",
    "        lines.append(line)\n",
    "\n",
    "    for line in lines:\n",
    "        writer.writerow(line.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En el dataset de Lagos (C), se creará una nueva columna llamada 'Sup Tamaño' que contendrá datos cualitativos indicando el tamaño de cada lago en función de su superficie en kilómetros cuadrados (km²). Esta columna se completará con las palabras: \"chico\", \"medio\" o \"grande\", según los siguientes criterios:\n",
    "#####\n",
    "1. Lagos con una superficie menor o igual a 17 km² serán clasificados como \"chico\".\n",
    "2. Lagos con una superficie mayor que 17 km² y menor o igual a 59 km² serán clasificados como \"medio\".\n",
    "3. Lagos con una superficie mayor a 59 km² serán clasificados como \"grande\".\n",
    "\n",
    "Además de transformar el campo de coordenadas actual, que sigue el formato estándar de grados, minutos y segundos (GMS), por ejemplo, 42°9'3\"S 71°38'59\"O, en dos campos separados para la latitud y longitud, añade dos nuevos campos para representar la latitud y longitud en formato de grados decimales (GD). Asegúrate de proporcionar la conversión correcta de GMS a GD para ambos campos. En el ejemplo mencionado el valor resultante debería ser -42.150833 para la latitud y -71.649722 para la longitud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDICE DATA LAGOS\n",
    "NOMBRE = 0\n",
    "UBICACION = 1\n",
    "SUPERFICIE = 2\n",
    "PROFUNDUDAD_MAXIMA = 3\n",
    "PROFUNDIDAD_MEDIA = 4\n",
    "CORDENADAS = 5\n",
    "\n",
    "read_dataset = pathlib.Path('../datasets/original_datasets/lagos_arg.csv')\n",
    "write_dataset = pathlib.Path('../datasets/custom_datasets/lagos_arg.csv')\n",
    "\n",
    "with(read_dataset.open(mode='r', encoding='utf-8') as read_file, \n",
    "        write_dataset.open(mode='w', newline=\"\", encoding='utf-8') as write_file):\n",
    "\n",
    "    reader = csv.reader(read_file)\n",
    "    writer = csv.writer(write_file)\n",
    "    # Me guardo la primera fila\n",
    "    header = next(reader)\n",
    "    # Elimino columna \"Coordenadas\"\n",
    "    header.pop()\n",
    "    # Le agrego al header las columna \"Latitud\" - \"Longitud\" - \"Sup tamaño\"\n",
    "    # uso .extend() para poder agregar todo en una linea\n",
    "    header.extend([\"Latitud\", \"Longitud\", \"Sup tamaño\"])\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Recorro fila por fila la data, y modifico lo necesario\n",
    "    for line in reader:\n",
    "        # Cordenadas (GMS a GD)\n",
    "        # Separo en latitud y longitud\n",
    "        lat_long = line[CORDENADAS].split(\" \")\n",
    "        lat_GD = dataset_module.convert_GMS_coordinate(lat_long[0])\n",
    "        long_GD = dataset_module.convert_GMS_coordinate(lat_long[1])\n",
    "        # Elimino la columna de datos de las coordenadas en GMS\n",
    "        line.pop()\n",
    "        # Agrego las columnas latitud y longitud\n",
    "        line.extend([lat_GD, long_GD])\n",
    "\n",
    "        # Dependiendo de su superficie agrego columna con tamaño\n",
    "        # \"Chico\" para sup<=17,\"medio\" 17<sup<=57 y \"grande\" >57\n",
    "        line.append('chico' if int(line[SUPERFICIE]) <= 17 else 'medio' if int(line[SUPERFICIE]) <= 59 else 'grande')\n",
    "        writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Para el dataset de Población del censo 2022 (D) reemplazar los valores \"///\" y \"-\" por cero en los campos que corresponda.\n",
    "\n",
    "Además agregar un nuevo campo que tenga el porcentaje de población en situación de calle. Tener en cuenta el total general (NO tener en cuenta los totales por sexo registrado al nacer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indice data censo\n",
    "total_poblacion = 1\n",
    "poblacion_en_situacion_de_calle = 4\n",
    "\n",
    "read_dataset = pathlib.Path('../datasets/original_datasets/c2022_tp_c_resumen_adaptado.csv')\n",
    "write_dataset = pathlib.Path('../datasets/custom_datasets/c2022_tp_c_resumen_adaptado.csv')\n",
    "\n",
    "with(read_dataset.open(mode='r',newline=\"\", encoding='utf-8') as read_file,\n",
    "     write_dataset.open(mode='w',newline=\"\", encoding='utf-8') as write_file):\n",
    "    \n",
    "    reader = csv.reader(read_file)\n",
    "    writer = csv.writer(write_file)\n",
    "    #Creo el header y agrego la columna 'Porcentaje de población en situación de calle'\n",
    "    header= next(reader)\n",
    "    header.append('Porcentaje de población en situación de calle')\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for line in reader:\n",
    "        #reemplazo en la linea '///' o '-' por 0\n",
    "        lines = dataset_module.replace_values(line)\n",
    "        #obtengo el porcentaje de poblacion en situacion de calle y lo agrego a la lista        \n",
    "        percentage= dataset_module.calculate_percentage(lines[total_poblacion],lines[poblacion_en_situacion_de_calle])\n",
    "        lines.append(percentage)\n",
    "        writer.writerow(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
